use candle_core::{Tensor, IndexOp, Result};
const MAX_ROWS: usize = 3;
const MAX_COLS: usize = 8;

pub trait TensorExt {
    // æ ‡å‡†æ–¹å·®
    fn std_all(&self, dim: Option<usize>, keepdim: bool) -> Result<Tensor>;
    // æ‰“å°å‘é‡ç»´åº¦, æ•°æ®, å¹³å‡æ–¹å·®, æ ‡å‡†æ–¹å·®, ä½™å¼¦ç›¸ä¼¼åº¦
    fn print(&self, name: &str, stats: bool) -> anyhow::Result<()>;
    // âœ¨ æ–°å¢ï¼šæ²¿æŒ‡å®šç»´åº¦æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰
    fn standardize(&self, dim: usize) -> Result<(Tensor, Tensor, Tensor)>;

    // âœ¨ æ–°å¢ï¼šç”¨ç»™å®š mean/std è¿˜åŸ
    fn unstandardize(&self, mean: &Tensor, std: &Tensor) -> Result<Tensor>;
}

// è¾…åŠ©å‡½æ•°ï¼šé€’å½’æ‰“å°é«˜ç»´å¼ é‡
fn print_high_dim_sample(
    tensor: &Tensor,
    name: &str,
    current_indices: &[usize],
    dims: &[usize],
    max_elements: usize,
    depth: usize,
) -> anyhow::Result<()> {
    if depth == dims.len() - 1 {
        // åˆ°è¾¾æœ€åä¸€ç»´ï¼Œæ‰“å°æ•°æ®
        let mut indices = current_indices.to_vec();
        indices.push(0); // ä¸´æ—¶ç´¢å¼•ï¼Œä¼šè¢«æ›¿æ¢

        // æ„å»ºç´¢å¼•è®¿é—®
        let slice = match current_indices.len() {
            0 => tensor.clone(),
            1 => tensor.i(current_indices[0])?,
            2 => tensor.i((current_indices[0], current_indices[1]))?,
            3 => tensor.i((current_indices[0], current_indices[1], current_indices[2]))?,
            4 => tensor.i((current_indices[0], current_indices[1], current_indices[2], current_indices[3]))?,
            _ => return Err(anyhow::anyhow!("Unsupported tensor rank > 5")),
        };

        let data = slice.to_vec1::<f32>()?;
        let display_count = data.len().min(max_elements);
        let display_data: Vec<f32> = data.iter().take(display_count).copied().collect();

        let indent = "  ".repeat(depth);
        let index_str = current_indices.iter()
            .map(|i| format!("[{}]", i))
            .collect::<String>();

        if data.len() > max_elements {
            println!("{}{}[..{}]: {:?}...", indent, index_str, display_count, display_data);
        } else {
            println!("{}{}: {:?}", indent, index_str, display_data);
        }
    } else {
        // è¿˜æ²¡åˆ°æœ€åä¸€ç»´ï¼Œç»§ç»­é€’å½’
        let elements_to_show = dims[depth].min(MAX_ROWS);

        for i in 0..elements_to_show {
            let mut new_indices = current_indices.to_vec();
            new_indices.push(i);
            print_high_dim_sample(tensor, name, &new_indices, dims, max_elements, depth + 1)?;
        }

        if dims[depth] > 2 {
            let indent = "    ".repeat(depth);
            //println!("{}... ({} more elements at this level)", indent, dims[depth] - 2);
            println!("{indent}...");
        }
    }

    Ok(())
}


fn cosine_sim_str(x: &Tensor) -> anyhow::Result<String> {
    // å¤„ç† [1, M, N] â†’ [M, N]
    let x = if let Ok((1, _, _)) = x.dims3() {
        x.squeeze(0)?
    } else {
        x.clone()
    };

    // å¿…é¡»æ˜¯ 2D ä¸”è‡³å°‘ä¸¤è¡Œ
    let (n_rows, _n_cols) = x.dims2().map_err(|_| anyhow::anyhow!("not 2D"))?;

    if n_rows < 2 {
        return Ok("".to_string());
    }

    let norms = x.sqr()?.sum_keepdim(1)?.sqrt()?;
    let eps = Tensor::full(1e-8f64, norms.shape(), x.device())?.to_dtype(x.dtype())?;
    let normalized = x.broadcast_div(&norms.maximum(&eps)?)?;
    let sim_matrix = normalized.matmul(&normalized.t()?)?;

    let sims = sim_matrix.to_vec2::<f32>()?;
    let mut sum = 0.0f32;
    let mut count = 0;
    for i in 0..n_rows {
        for j in (i + 1)..n_rows {
            sum += sims[i][j];
            count += 1;
        }
    }

    if count == 0 {
        Ok("".to_string())
    } else {
        Ok(format!("{:.4}", sum / count as f32))
    }
}

impl TensorExt for Tensor {
    fn std_all(&self, dim: Option<usize>, keepdim: bool) -> Result<Tensor> {
        // è®¡ç®—å‡å€¼
        let mean = if let Some(d) = dim {
            self.mean_keepdim(d)?
        } else {
            self.mean_all()?
        };

        // è®¡ç®—æ–¹å·® (x - mean)^2 çš„å‡å€¼
        let diff = self.broadcast_sub(&mean)?;
        let squared_diff = diff.sqr()?;
        let variance = if let Some(d) = dim {
            if keepdim {
                squared_diff.mean_keepdim(d)?
            } else {
                squared_diff.mean(d)?
            }
        } else {
            squared_diff.mean_all()?
        };

        // è®¡ç®—æ ‡å‡†å·® (æ–¹å·®çš„å¹³æ–¹æ ¹)
        variance.sqrt()
    }

    fn print(&self, name: &str, stats: bool) -> anyhow::Result<()> {
        let stats_info = if stats {
            let mean_all = self.mean_all()?.to_vec0::<f32>()?;
            let std_all = self.std_all(None, true)?.to_vec0::<f32>()?;
            let cosim = cosine_sim_str(self)?;
            format!(" (mean: {:.4}, std: {:.4}{})",mean_all, std_all,
                    (!cosim.is_empty()).then(||format!(", cossim: {cosim}")).unwrap_or_default())
        } else {
            "".to_string()
        };
        let shape = self.shape();

        // åªå¤„ç† F32 ç±»å‹
        if self.dtype() != candle_core::DType::F32 {
            println!("{name}: [unsupported dtype for display]");
            return Ok(());
        }

        match shape.rank() {
            0 => {
                // æ ‡é‡
                let val = self.to_scalar::<f32>()?;
                println!("{name}: {}", val);
            }
            1 => {
                // 1ç»´å¼ é‡ï¼šæœ€å¤šæ‰“å° max_elements ä¸ªå…ƒç´ 
                let data = self.to_vec1::<f32>()?;
                let display_count = data.len().min(MAX_COLS);
                let display_data: Vec<f32> = data.iter().take(display_count).copied().collect();
                if data.len() > MAX_COLS {
                    println!("{name}[..{}]:{stats_info} {:?}...", display_count, display_data);
                } else {
                    println!("{name}:{stats_info} {:?}", display_data);
                }
            }
            2 => {
                // 2ç»´å¼ é‡ï¼šç¬¬ä¸€ç»´æœ€å¤šæ‰“å°3ä¸ªï¼Œæœ€åä¸€ç»´æœ€å¤šæ‰“å° MAX_COLS ä¸ª
                let dims = shape.dims();
                let rows_to_show = dims[0].min(MAX_ROWS);

                println!("{name}{:?}:{stats_info}", dims);
                for i in 0..rows_to_show {
                    let row = self.i(i)?;
                    let row_data = row.to_vec1::<f32>()?;
                    let display_count = row_data.len().min(MAX_COLS);
                    let display_data: Vec<f32> = row_data.iter().take(display_count).copied().collect();

                    if row_data.len() > MAX_COLS {
                        println!("  [{i}][..{}]: {:?}...", display_count, display_data);
                    } else {
                        println!("  [{i}]: {:?}", display_data);
                    }
                }

                if dims[0] > 3 {
                    println!("  ... ({} more rows)", dims[0] - 3);
                }
            }
            _ => {
                // é«˜ç»´å¼ é‡ï¼ˆ>2ç»´ï¼‰ï¼šå‰é¢å‡ ç»´æœ€å¤šæ‰“å°2ä¸ªï¼Œæœ€åä¸€ç»´æœ€å¤šæ‰“å° MAX_COLS ä¸ª
                let dims = shape.dims();
                println!("{name}{:?}:{stats_info}", dims);

                // é€’å½’æ‰“å°é«˜ç»´å¼ é‡çš„å‰2ä¸ªå…ƒç´ 
                print_high_dim_sample(self, name, &[], dims, MAX_COLS, 0)?;
            }
        }

        Ok(())
    }

    // ğŸ”§ æ ‡å‡†åŒ–ï¼šè¿”å› (normalized, mean, std)
    // let (targets_norm, mean, std) = targets.standardize(0)?;
    fn standardize(&self, dim: usize) -> Result<(Tensor, Tensor, Tensor)> {
        let mean = self.mean(dim)?;
        let centered = self.broadcast_sub(&mean)?;
        let var = centered.sqr()?.mean(dim)?;
        let std = var.sqrt()?;

        // é˜²æ­¢é™¤é›¶
        let eps = 1e-8f32;
        let device = std.device();
        let eps_tensor = Tensor::new(eps, device)?.broadcast_as(std.shape())?;
        let std = std.maximum(&eps_tensor)?;

        let normalized = centered.broadcast_div(&std)?;
        Ok((normalized, mean, std))
    }

    // ğŸ”§ è¿˜åŸæ ‡å‡†åŒ–
    // let final_pred = final_pred_norm.unstandardize(&mean, &std)?;
    fn unstandardize(&self, mean: &Tensor, std: &Tensor) -> Result<Tensor> {
        let scaled = self.broadcast_mul(std)?;
        scaled.broadcast_add(mean)
    }
}


// use candle_core::{Device, DType};
/// å¯¹ [N, D] å¼ é‡æ²¿ç¬¬0ç»´ï¼ˆæ ·æœ¬ç»´åº¦ï¼‰åš Z-score æ ‡å‡†åŒ–
/// è¿”å› (normalized_tensor, mean, std)

#[cfg(test)]
mod tests {
    use super::*;
    use candle_core::{DType, Device};
    #[test]
    fn test_print_tensor() {
        let tensor1 = Tensor::zeros((5, 10, 10), DType::F32, &Device::Cpu).unwrap();
        tensor1.print("test", false).unwrap();
    }
    #[test]
    fn test_print_sim() -> anyhow::Result<()> {
        let device = Device::Cpu;

        // æ„é€ ä¸¤ä¸ªæ˜æ˜¾ä¸åŒçš„å‘é‡ï¼šä¸€ä¸ªåæ­£ï¼Œä¸€ä¸ªåè´Ÿ
        let row1 = vec![1.0f32, 22.0, 13.0, 5.0];
        let row2 = vec![-1.0, -22.0, -13.0, -5.0];
        // let row2 = vec![-11.0f32, -21.0, -3.0, 400.0];
        let data = [row1, row2].concat(); // [1,2,3,4,-1,-2,-3,-4]

        // æµ‹è¯• 2D: [2, 4]
        let tensor_2d = Tensor::from_vec(data.clone(), (2, 4), &device)?;
        //println!("--- Testing 2D input ---");
        tensor_2d.print("2D", true)?;

        // æµ‹è¯• 3D with batch=1: [1, 2, 4]
        let tensor_3d = Tensor::from_vec(data, (1, 2, 4), &device)?;
        //println!("\n--- Testing [1,2,4] input ---");
        tensor_3d.print("3D", true)?;

        Ok(())
    }
    #[test]
    fn test_std_all() -> Result<()> {
        let device = &Device::Cpu;

        let data = [[1.0f32, 2.0, 3.0], [4.0, 5.0, 6.0]];
        let flat: Vec<f32> = data.iter().flatten().copied().collect();
        let tensor = Tensor::from_slice(&flat, (2, 3), device)?;

        // 1. å…¨å±€æ ‡å‡†å·® â†’ scalar (rank 0)
        let std_global = tensor.std_all(None, false)?;
        let std_global_val = std_global.to_scalar::<f32>()?;
        assert!((std_global_val - 1.7078251).abs() < 1e-5);

        // 2. æ²¿ dim=1ï¼Œkeepdim=true â†’ shape [2, 1]
        let std_dim1_keepdim = tensor.std_all(Some(1), true)?;
        assert_eq!(std_dim1_keepdim.dims(), &[2, 1]);
        let vals = std_dim1_keepdim.to_vec2::<f32>()?;
        assert!((vals[0][0] - 0.8164966).abs() < 1e-5);
        assert!((vals[1][0] - 0.8164966).abs() < 1e-5);

        // 3. æ²¿ dim=0ï¼Œkeepdim=false â†’ shape [3]
        let std_dim0_no_keepdim = tensor.std_all(Some(0), false)?;
        assert_eq!(std_dim0_no_keepdim.dims(), &[3]);
        let vals = std_dim0_no_keepdim.to_vec1::<f32>()?;
        for v in vals {
            assert!((v - 1.5).abs() < 1e-5);
        }

        Ok(())
    }
}